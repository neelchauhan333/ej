pr2 
1) ./start-dfs.sh
 2) ./start-yarn.sh
 3)Jps
 4) mkdir wordcount
 5) cd ~/wordcount
 6) nano reducer.py
 7) Crtl+ x to save and then press Y to save
 8)nano mapper.py
import sys
for line in sys.stdin:
  line = line.strip().lower()
  words = line.strip()
  for word in words:
    If word:
      print(f" (word)\t1")

 6) nano reducer.py

import sys
current_word = None
current count = 0
for line in sys.stdin:
line line.strip()
try:
word, count = line.split('\t',1)
count = int(count)
except:
continue
if current_word == word:
current_count += count
else:
if current_word:
print(f"{current_word}\t{current_count}")
current_count = count
current_word = word
if current_word is not None:
print(f"{current_word}\t{current_count}")

input.txt
 echo-e "hello world\nhello hadoop" > input.txt
 Upload Files to HDFS
 hdfs dfs-mkdir-p /user/yourusername/wordcount
 hdfs dfs-put input.txt /user/yourusername/wordcount/
 # Make sure you are in the directory containing mapper.py and reducer.py cd
 ~/wordcount/ # The command to run the streaming job hadoop jar
 hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar
 \-input /user/gayatri/wordcount/input.txt \
-output /user/gayatri/wordcount/output \-mapper shell_mapper.sh \-reducer reducer.py \-file shell_mapper.sh \-file reducer.py
 hadoop fs-cat /user/gayatri/wordcount/output/part-00000






pr3
Steps: Prepare Your Hadoop Environment
 1)start-dfs.sh
 2)start-yarn.sh
 3)jps
 4) mkdir weather
 5)cd weather
 6)nano mapper.py
 if not already done.
 7)chmod +x mapper.py reducer.py
 8)nano mapper.py
 9)nano reducer.py
 #!/usr/bin/env python3

 import sys
 current_year = None
 max_temp = float('-inf')
 for line in sys.stdin:
 line = line.strip()
 if not line:
 continue
 # Split the input from the mapper by the tab character
 try:
 year, temp_str = line.split('\t')
 temp = int(temp_str)
 except ValueError:
 continue
 except:
 continue
 if current_year == year:
 if temp > max_temp:
 max_temp = temp
 else:
 if current_year is not None:
 print(f"{current_year}\t{max_temp}")
 current_year = year
 max_temp = temp
 # Output the last year
 if current_year is not None:
 print(f"{current_year}\t{max_temp}")

echo-e "198101011200 23\n198101021200 30\n198101031200
5\n198201011200 50\n198201021200 35" > weather.txt
hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar
 \-input /user/gayatri/weather/weather.txt \-output /user/username/weather/output \-mapper mapper.py \-reducer reducer.py \-file mapper.py \-file reducer.py

 hdfs dfs-cat /user/username/weather/output/part-00000





pr4
from pymongo import MongoClient
from collections import Counter

client = MongoClient("mongodb://localhost:27017/")
db =client["MyDatabase"]

collection = db["MyCollection"]

data = [
 {"tag": "A", "value": 10},
 {"tag": "A", "value": 20},
 {"tag": "B", "value": 15},
 {"tag": "C", "value": 5},
 {"tag": "A", "value": 25},
 {"tag": "B", "value": 30},
 ]

collection.insert_many(data)

max_doc = collection.find({"tag": "A"}).sort("value",-1).limit(1)
for doc in max_doc:
 print("Max value with tag A:", doc)

 tags = [doc["tag"] for doc in collection.find({}, {"tag": 1})]
 print("Tag counts:", Counter(tags))







practical5
 Implement a Big Data Application that:
 Stores large-scale data in MongoDB ("House MongoDB")
 Manipulates the data using Pig (Apache Pig Latin language)
 1)./start-dfs.sh
 2)./start-yarn.sh
 3)Jps
 4)nano mongodb_app.py
 5)python3 nano mongodb_app.py







Practical 6 Configure the Hive and implement the application in Hive.
 1)cd /usr/local/hive/conf
 2) ./start-dfs.sh
 3)./start-yarn.sh
 4)Jps
 5)cd ~
 OPEN second terminal nano employees.txt
Ctrl+x press to save
 To write ctrl+o
 7)
 8) schematool-initSchema-dbType derby
9)hive
 10)CREATE TABLE employees (idINT, name STRING,age
 INT,department STRING)
 11)ROW FORMAT DELIMITED FIELDS TERMINATEDBY ‘,’;
 12) LOAD DATAINPATH‘/user/hive/data/employees.txt’ INTO TABLE
 employees;
 13)SELECT * FROMemployees;








pr7 DTC
from sklearn.datasets import load_iris 
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import classification_report, accuracy_score 
import matplotlib.pyplot as plt 

iris = load_iris() 
X = iris.data  
y = iris.target   

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) 

model = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0) 
model.fit(X_train, y_train) 

y_pred = model.predict(X_test) 

print("Classification Report:\n", classification_report(y_test, y_pred)) 
print("Accuracy:", accuracy_score(y_test, y_pred)) 

print("\nDecision Tree Rules:\n") 
print(export_text(model, feature_names=iris.feature_names)) 

plt.figure(figsize=(12, 8)) 
plot_tree(model, feature_names=iris.feature_names, class_names=iris.target_names, filled=True) 
plt.title("Decision Tree Visualization") 
plt.show() 






pr8 SVM
from sklearn import datasets 
from sklearn.model_selection import train_test_split 
from sklearn.svm import SVC 
from sklearn.metrics import classification_report, accuracy_score 
import matplotlib.pyplot as plt 
import seaborn as sns 

iris = datasets.load_iris() 
X = iris.data 
y = iris.target 

X_train, X_test, y_train, y_test = train_test_split( 
X, y, test_size=0.3, random_state=42 
) 

model = SVC(kernel='linear')  
model.fit(X_train, y_train) 

y_pred = model.predict(X_test) 

print("Accuracy:", accuracy_score(y_test, y_pred)) 
print("\nClassification Report:\n", classification_report(y_test, y_pred)) 

from sklearn.metrics import confusion_matrix 
import numpy as np 
cm = confusion_matrix(y_test, y_pred) 
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
xticklabels=iris.target_names, 
yticklabels=iris.target_names) 
plt.title("Confusion Matrix") 
plt.xlabel("Predicted") 
plt.ylabel("Actual") 
plt.show()

